{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction SRT Cookbook Useful Links UDT Documentation","title":"Introduction"},{"location":"#introduction","text":"SRT Cookbook","title":"Introduction"},{"location":"#useful-links","text":"UDT Documentation","title":"Useful Links"},{"location":"faq/","text":"FAQ What applications support SRT? Open source: srt-live-transmit, FFmpeg, WireShark, GStreamer Corporate: Haivision Media Gateway, Haivision KB, Haivision Makito X I get error messages \"tsbpd wrap period begins/ends\". What does it mean? Messages about TSBPD wrap period are informational. These are not errors. SRT just informs that the timer used to track packets is close to overflow and is ready to be reset. There is nothing you should do about that. It is just to inform about this situation. For more information please see the TSBPD Wrapping Period section. Issues on GitHub: #642 . I get error messages \"No room to store incoming packet...\" What does it mean? This error message is usually related to the buffer sizes. See also #703. The Default receiver buffer size is approximately 96 Mbits. This buffer should store all the packets within the specified latency, and also have some space for the application to read (FFmpeg). You might probably need to increase the buffer size, by appending to the following URI \"srt://0.0.0.0:9999?pkt_size=1316&mode=listener\" the following querry values can be added: sndbuf - Send buffer size (in bytes) rcvbuf - Receive buffer size (in bytes) Other FFmpeg SRT URI options can be found here .","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#what-applications-support-srt","text":"Open source: srt-live-transmit, FFmpeg, WireShark, GStreamer Corporate: Haivision Media Gateway, Haivision KB, Haivision Makito X","title":"What applications support SRT?"},{"location":"faq/#i-get-error-messages-tsbpd-wrap-period-beginsends-what-does-it-mean","text":"Messages about TSBPD wrap period are informational. These are not errors. SRT just informs that the timer used to track packets is close to overflow and is ready to be reset. There is nothing you should do about that. It is just to inform about this situation. For more information please see the TSBPD Wrapping Period section. Issues on GitHub: #642 .","title":"I get error messages \"tsbpd wrap period begins/ends\". What does it mean?"},{"location":"faq/#i-get-error-messages-no-room-to-store-incoming-packet-what-does-it-mean","text":"This error message is usually related to the buffer sizes. See also #703. The Default receiver buffer size is approximately 96 Mbits. This buffer should store all the packets within the specified latency, and also have some space for the application to read (FFmpeg). You might probably need to increase the buffer size, by appending to the following URI \"srt://0.0.0.0:9999?pkt_size=1316&mode=listener\" the following querry values can be added: sndbuf - Send buffer size (in bytes) rcvbuf - Receive buffer size (in bytes) Other FFmpeg SRT URI options can be found here .","title":"I get error messages \"No room to store incoming packet...\" What does it mean?"},{"location":"apps/ffmpeg/","text":"FFmpeg FFmpeg supports SRT protocol out of the box. But it needs to be built with -enable-protocol=libsrt to include SRT. Refer to FFmpeg's Compilation Guide for build instructions for your platform. A list of the available protocols can be determined by calling ffmpeg -protocols . \"srt\" (or \"libsrt\") should be listed both as an input, and as an output protocol. Once SRT is available, an SRT output can be specified like this: \"srt://<destination_ip>:<destination_port>\" It would be a good idea to use MPEG-TS as a container -f mpegts . ffplay should be able to play SRT streaming with ffplay srt://<ip>:<port> . Piping can be used as well like this: srt-live-transmit srt://<ip>:<port> file://con | ffplay -f mpegts - Please note the issue #407 . Or pipe via UDP socket on a localhost: srt-live-transmit srt://<ip>:<port> udp://127.0.0.1:<portB> ffplay udp://127.0.0.1:<portB> -f mpegts In the latest case the -f mpegts argument is optional. Note! -re option will slow down the reading: Read input at native frame rate. Mainly used to simulate a grab device, or live input stream (e.g. when reading from a file). Should not be used with ... live input streams (where it can cause packet loss). By default ffmpeg attempts to read the input(s) as fast as possible. This option will slow down the reading of the input(s) to the native frame rate of the input(s). It is useful for real-time output (e.g. live streaming). FFmpeg example grabbing X11 desktop screen and streaming to SRT ffmpeg -f x11grab -follow_mouse centered -r 25 -s cif -i :0.0 \\ -f mpegts srt://<target_ip>:<target_port> FFmpeg example with SMPTE bars test video source ffmpeg -f lavfi -re -i smptebars=duration=60:size=1280x720:rate=30 -f lavfi -re \\ -i sine=frequency=1000:duration=60:sample_rate=44100 -pix_fmt yuv420p \\ -c:v libx264 -b:v 1000k -g 30 -keyint_min 120 -profile:v baseline \\ -preset veryfast -f mpegts \"srt://127.0.0.1:4200?pkt_size=1316\" Example with FFmpeg and srt-live-transmit Send to UDP localhost port 5000 ffmpeg -f lavfi -re -i smptebars=duration=60:size=1280x720:rate=30 -f lavfi -re \\ -i sine=frequency=1000:duration=60:sample_rate=44100 -pix_fmt yuv420p \\ -c:v libx264 -b:v 1000k -g 30 -keyint_min 120 -profile:v baseline \\ -preset veryfast -f mpegts \"udp://127.0.0.1:5000?pkt_size=1316\" Transmit from UDP to SRT destination ./srt-live-transmit udp://:5000 srt://<ip>:<port>?mode=rendezvous Receive with SRT ./srt-live-transmit srt://<ip>:<port>?mode=rendezvous file://con > ./received.ts Receive srt and pass to dev/nul `./ffmpeg -i srt://<ip>:<port> -f null /dev/null`","title":"FFmpeg"},{"location":"apps/ffmpeg/#ffmpeg","text":"FFmpeg supports SRT protocol out of the box. But it needs to be built with -enable-protocol=libsrt to include SRT. Refer to FFmpeg's Compilation Guide for build instructions for your platform. A list of the available protocols can be determined by calling ffmpeg -protocols . \"srt\" (or \"libsrt\") should be listed both as an input, and as an output protocol. Once SRT is available, an SRT output can be specified like this: \"srt://<destination_ip>:<destination_port>\" It would be a good idea to use MPEG-TS as a container -f mpegts . ffplay should be able to play SRT streaming with ffplay srt://<ip>:<port> . Piping can be used as well like this: srt-live-transmit srt://<ip>:<port> file://con | ffplay -f mpegts - Please note the issue #407 . Or pipe via UDP socket on a localhost: srt-live-transmit srt://<ip>:<port> udp://127.0.0.1:<portB> ffplay udp://127.0.0.1:<portB> -f mpegts In the latest case the -f mpegts argument is optional. Note! -re option will slow down the reading: Read input at native frame rate. Mainly used to simulate a grab device, or live input stream (e.g. when reading from a file). Should not be used with ... live input streams (where it can cause packet loss). By default ffmpeg attempts to read the input(s) as fast as possible. This option will slow down the reading of the input(s) to the native frame rate of the input(s). It is useful for real-time output (e.g. live streaming).","title":"FFmpeg"},{"location":"apps/ffmpeg/#ffmpeg-example-grabbing-x11-desktop-screen-and-streaming-to-srt","text":"ffmpeg -f x11grab -follow_mouse centered -r 25 -s cif -i :0.0 \\ -f mpegts srt://<target_ip>:<target_port>","title":"FFmpeg example grabbing X11 desktop screen and streaming to SRT"},{"location":"apps/ffmpeg/#ffmpeg-example-with-smpte-bars-test-video-source","text":"ffmpeg -f lavfi -re -i smptebars=duration=60:size=1280x720:rate=30 -f lavfi -re \\ -i sine=frequency=1000:duration=60:sample_rate=44100 -pix_fmt yuv420p \\ -c:v libx264 -b:v 1000k -g 30 -keyint_min 120 -profile:v baseline \\ -preset veryfast -f mpegts \"srt://127.0.0.1:4200?pkt_size=1316\"","title":"FFmpeg example with SMPTE bars test video source"},{"location":"apps/ffmpeg/#example-with-ffmpeg-and-srt-live-transmit","text":"Send to UDP localhost port 5000 ffmpeg -f lavfi -re -i smptebars=duration=60:size=1280x720:rate=30 -f lavfi -re \\ -i sine=frequency=1000:duration=60:sample_rate=44100 -pix_fmt yuv420p \\ -c:v libx264 -b:v 1000k -g 30 -keyint_min 120 -profile:v baseline \\ -preset veryfast -f mpegts \"udp://127.0.0.1:5000?pkt_size=1316\" Transmit from UDP to SRT destination ./srt-live-transmit udp://:5000 srt://<ip>:<port>?mode=rendezvous Receive with SRT ./srt-live-transmit srt://<ip>:<port>?mode=rendezvous file://con > ./received.ts Receive srt and pass to dev/nul `./ffmpeg -i srt://<ip>:<port> -f null /dev/null`","title":"Example with FFmpeg and srt-live-transmit"},{"location":"apps/gstreamer-plugin/","text":"GStreamer Starting from ver. 1.14 GStreamer supports SRT (see the v.1.14 release notes ). See the SRT plugin for GStreamer on git . Using GStreamer and SRT to set up a screensharing Based on the description in #7 . Note that the commands are likely to change slightly for gstreamer 1.16 (see this issue ). If you don't want to build GSteamer, SRT, and all the plugins from source or don't have a distribution that has 1.14 readily available, you can use nix to reproduce what is shown further. Simply install nix ; then use the command bellow to open a shell where the following commands work. NIX_PATH=nixpkgs=https://github.com/nh2/nixpkgs/archive/a94ff5f6aaa.tar.gz \\ nix-shell -p gst_all_1.gstreamer \\ -p gst_all_1.gst-plugins-good -p gst_all_1.gst-plugins-base \\ -p gst_all_1.gst-plugins-bad \\ -p gst_all_1.gst-plugins-ugly -p gst_all_1.gst-libav Sender server Set up a sender server that will grab a source raw video from a desktop or a webcam, encode it with x.264 (H.264/AVC) encoder, pack it in MPEG-TS ( more info about live streaming ). Then pipe it into the SRT sink that sends it over the network to the receiver client. The streaming URI should looks like uri=srt://<ip>:<port> . In the examples below the streaming is sent to port 888 on a localhost by specifying uri=srt://0.0.0.0:8888 . For screensharing (Linux with X Display) The ximagesrc GStreamer plugin can be used to capture X Display and create raw RGB video. Refer to ximagesrc RM for configuration options. /usr/bin/time gst-launch-1.0 ximagesrc startx=0 show-pointer=true use-damage=0 \\ ! videoconvert \\ ! x264enc bitrate=32000 tune=zerolatency speed-preset=veryfast \\ byte-stream=true threads=1 key-int-max=15 \\ intra-refresh=true ! video/x-h264, profile=baseline, framerate=30/1 ! mpegtsmux \\ ! srtserversink uri=srt://0.0.0.0:8888/ latency=100 For webcam images The v4l2src GStreamer plugin can be used to capture video from v4l2 devices, like webcams and TV cards. Refer to v4l2src RM for further information. /usr/bin/time gst-launch-1.0 v4l2src ! videoconvert \\ ! x264enc bitrate=8000 tune=zerolatency speed-preset=superfast \\ byte-stream=true threads=1 key-int-max=15 intra-refresh=true \\ ! video/x-h264, profile=baseline ! mpegtsmux \\ ! srtserversink uri=srt://0.0.0.0:8888/ latency=100 Notes The decodebin can also be used to configure settings automatically. Using explicit pipeline elements here make it possible to tune the settings when needed. A use of time helps to determine when the thread is capped at 100%, while the the thread=1 parameter makes the encoding use only one thread. Remove threads=1 to allow multiple cores, or cjange the speed-preset to reduce CPU load. The timeout setting can be tuned. A recommended timeout is 2x-2.5x of the expected roundtrip time. The password functionality works as well, but only if a password is >= 10 characters long; otherwise it's completely ignored. See this bug of GStreamer. Receiver client A client connection over SRT to the server with URI srt://127.0.0.1:8888 (localhost) or a remote server is set up. URI syntax is srt://<ip>:<port> . Then MPEG-TS demuxer and video decoder is used to get a decompressed video, that goes to a playback plugin autovideosink . Note that multiple clients can connect to the server started earlier. gst-launch-1.0 srtclientsrc uri=srt://127.0.0.1:8888 ! tsdemux ! h264parse ! video/x-h264 ! avdec_h264 ! autovideosink sync=false This works over both the internet and localhost. Useful Links Oliver Cr\u00eate. SRT in GStreamer . 2018","title":"GStreamer"},{"location":"apps/gstreamer-plugin/#gstreamer","text":"Starting from ver. 1.14 GStreamer supports SRT (see the v.1.14 release notes ). See the SRT plugin for GStreamer on git .","title":"GStreamer"},{"location":"apps/gstreamer-plugin/#using-gstreamer-and-srt-to-set-up-a-screensharing","text":"Based on the description in #7 . Note that the commands are likely to change slightly for gstreamer 1.16 (see this issue ). If you don't want to build GSteamer, SRT, and all the plugins from source or don't have a distribution that has 1.14 readily available, you can use nix to reproduce what is shown further. Simply install nix ; then use the command bellow to open a shell where the following commands work. NIX_PATH=nixpkgs=https://github.com/nh2/nixpkgs/archive/a94ff5f6aaa.tar.gz \\ nix-shell -p gst_all_1.gstreamer \\ -p gst_all_1.gst-plugins-good -p gst_all_1.gst-plugins-base \\ -p gst_all_1.gst-plugins-bad \\ -p gst_all_1.gst-plugins-ugly -p gst_all_1.gst-libav","title":"Using GStreamer and SRT to set up a screensharing"},{"location":"apps/gstreamer-plugin/#sender-server","text":"Set up a sender server that will grab a source raw video from a desktop or a webcam, encode it with x.264 (H.264/AVC) encoder, pack it in MPEG-TS ( more info about live streaming ). Then pipe it into the SRT sink that sends it over the network to the receiver client. The streaming URI should looks like uri=srt://<ip>:<port> . In the examples below the streaming is sent to port 888 on a localhost by specifying uri=srt://0.0.0.0:8888 .","title":"Sender server"},{"location":"apps/gstreamer-plugin/#for-screensharing-40linux-with-x-display41","text":"The ximagesrc GStreamer plugin can be used to capture X Display and create raw RGB video. Refer to ximagesrc RM for configuration options. /usr/bin/time gst-launch-1.0 ximagesrc startx=0 show-pointer=true use-damage=0 \\ ! videoconvert \\ ! x264enc bitrate=32000 tune=zerolatency speed-preset=veryfast \\ byte-stream=true threads=1 key-int-max=15 \\ intra-refresh=true ! video/x-h264, profile=baseline, framerate=30/1 ! mpegtsmux \\ ! srtserversink uri=srt://0.0.0.0:8888/ latency=100","title":"For screensharing (Linux with X Display)"},{"location":"apps/gstreamer-plugin/#for-webcam-images","text":"The v4l2src GStreamer plugin can be used to capture video from v4l2 devices, like webcams and TV cards. Refer to v4l2src RM for further information. /usr/bin/time gst-launch-1.0 v4l2src ! videoconvert \\ ! x264enc bitrate=8000 tune=zerolatency speed-preset=superfast \\ byte-stream=true threads=1 key-int-max=15 intra-refresh=true \\ ! video/x-h264, profile=baseline ! mpegtsmux \\ ! srtserversink uri=srt://0.0.0.0:8888/ latency=100","title":"For webcam images"},{"location":"apps/gstreamer-plugin/#notes","text":"The decodebin can also be used to configure settings automatically. Using explicit pipeline elements here make it possible to tune the settings when needed. A use of time helps to determine when the thread is capped at 100%, while the the thread=1 parameter makes the encoding use only one thread. Remove threads=1 to allow multiple cores, or cjange the speed-preset to reduce CPU load. The timeout setting can be tuned. A recommended timeout is 2x-2.5x of the expected roundtrip time. The password functionality works as well, but only if a password is >= 10 characters long; otherwise it's completely ignored. See this bug of GStreamer.","title":"Notes"},{"location":"apps/gstreamer-plugin/#receiver-client","text":"A client connection over SRT to the server with URI srt://127.0.0.1:8888 (localhost) or a remote server is set up. URI syntax is srt://<ip>:<port> . Then MPEG-TS demuxer and video decoder is used to get a decompressed video, that goes to a playback plugin autovideosink . Note that multiple clients can connect to the server started earlier. gst-launch-1.0 srtclientsrc uri=srt://127.0.0.1:8888 ! tsdemux ! h264parse ! video/x-h264 ! avdec_h264 ! autovideosink sync=false This works over both the internet and localhost.","title":"Receiver client"},{"location":"apps/gstreamer-plugin/#useful-links","text":"Oliver Cr\u00eate. SRT in GStreamer . 2018","title":"Useful Links"},{"location":"apps/obs-studio/","text":"OBS Studio OBS Studio is a free and open source software for video recording and live streaming. Starting from version ?? It supports SRT.","title":"OBS Studio"},{"location":"apps/obs-studio/#obs-studio","text":"OBS Studio is a free and open source software for video recording and live streaming. Starting from version ?? It supports SRT.","title":"OBS Studio"},{"location":"apps/vlc-media-player/","text":"VLC Media Player VLC media player supports SRT as an input (starting from version 3.0?). SRT access module source code: link . To open an SRT stream go to the menu \"Media\" -> \"Open Network Stream\". Specfy the source URI in the format: srt://ip:port . Note that VLC does not parse URI query options, so the parameters passed in a query are ignored. For example, srt://ip:port?passphrase=123456789!@ is identical to a simple srt:ip:port . SRT options can be specified on the corresponding property page. Go to the menu \"Tools\" -> \"Preferences\". In the bottom-left corner select \"Show Settings: All\". Select \"SRT\" in the tree node of \"Input Codecs\" -> \"Access modules\".","title":"VLC Media Player"},{"location":"apps/vlc-media-player/#vlc-media-player","text":"VLC media player supports SRT as an input (starting from version 3.0?). SRT access module source code: link . To open an SRT stream go to the menu \"Media\" -> \"Open Network Stream\". Specfy the source URI in the format: srt://ip:port . Note that VLC does not parse URI query options, so the parameters passed in a query are ignored. For example, srt://ip:port?passphrase=123456789!@ is identical to a simple srt:ip:port . SRT options can be specified on the corresponding property page. Go to the menu \"Tools\" -> \"Preferences\". In the bottom-left corner select \"Show Settings: All\". Select \"SRT\" in the tree node of \"Input Codecs\" -> \"Access modules\".","title":"VLC Media Player"},{"location":"apps/wireshark/","text":"Wireshark","title":"Wireshark"},{"location":"apps/wireshark/#wireshark","text":"","title":"Wireshark"},{"location":"getting-started/build-on-windows/","text":"Build on Windows Using NuGet 1. Download and install OpenSSL for Windows. The 64-bit package can be downloaded from here: Win64OpenSSL_Light-1_1_1c.exe . (Note that the last letter or version number may be changed and older versions no longer available. If this isn't found, check here: http://slproweb.com/products/Win32OpenSSL.html ) It's expected to be installed in C:\\Program Files\\OpenSSL-Win64 . Add this path to the user's or system's environment variable PATH . 2. Install Pthreads for Windows nuget install cinegy.pthreads-win64 -version 2.9.1.17 -OutputDirectory C:\\pthread-win32 3. Install cmake for Windows. CMake dowload page . The CMake GUI will help you configure the project. 4. Generate Visual Studio Solution. Assuming you are currently in the cloned repo. mkdir _build & cd _build cmake ../ -G\"Visual Studio 16 2019\" -A x64 -DPTHREAD_INCLUDE_DIR=\"C:\\pthread-win32\\cinegy.pthreads-win64.2.9.1.17\\sources\" -DPTHREAD_LIBRARY=\"C:\\pthread-win32\\cinegy.pthreads-win64.2.9.1.17\\runtimes\\win-x64\\native\\release\\pthread_lib.lib\" cmake --build ./ --config Release CMake will try to find OpenSSL and pthreads. If any of the is not found, you can define the following variables to help CMake find them: OPENSSL_ROOT_DIR=<path to OpenSSL installation> OPENSSL_LIBRARIES=<path to all the openssl libraries to link> OPENSSL_INCLUDE_DIR=<path to the OpenSSL include dir> PTHREAD_INCLUDE_DIR=<path to where pthread.h lies> PTHREAD_LIBRARY=<path to pthread.lib> Using vcpkg","title":"Build on Windows"},{"location":"getting-started/build-on-windows/#build-on-windows","text":"","title":"Build on Windows"},{"location":"getting-started/build-on-windows/#using-nuget","text":"","title":"Using NuGet"},{"location":"getting-started/build-on-windows/#1-download-and-install-openssl-for-windows","text":"The 64-bit package can be downloaded from here: Win64OpenSSL_Light-1_1_1c.exe . (Note that the last letter or version number may be changed and older versions no longer available. If this isn't found, check here: http://slproweb.com/products/Win32OpenSSL.html ) It's expected to be installed in C:\\Program Files\\OpenSSL-Win64 . Add this path to the user's or system's environment variable PATH .","title":"1. Download and install OpenSSL for Windows."},{"location":"getting-started/build-on-windows/#2-install-pthreads-for-windows","text":"nuget install cinegy.pthreads-win64 -version 2.9.1.17 -OutputDirectory C:\\pthread-win32","title":"2. Install Pthreads for Windows"},{"location":"getting-started/build-on-windows/#3-install-cmake-for-windows","text":"CMake dowload page . The CMake GUI will help you configure the project.","title":"3. Install cmake for Windows."},{"location":"getting-started/build-on-windows/#4-generate-visual-studio-solution","text":"Assuming you are currently in the cloned repo. mkdir _build & cd _build cmake ../ -G\"Visual Studio 16 2019\" -A x64 -DPTHREAD_INCLUDE_DIR=\"C:\\pthread-win32\\cinegy.pthreads-win64.2.9.1.17\\sources\" -DPTHREAD_LIBRARY=\"C:\\pthread-win32\\cinegy.pthreads-win64.2.9.1.17\\runtimes\\win-x64\\native\\release\\pthread_lib.lib\" cmake --build ./ --config Release CMake will try to find OpenSSL and pthreads. If any of the is not found, you can define the following variables to help CMake find them: OPENSSL_ROOT_DIR=<path to OpenSSL installation> OPENSSL_LIBRARIES=<path to all the openssl libraries to link> OPENSSL_INCLUDE_DIR=<path to the OpenSSL include dir> PTHREAD_INCLUDE_DIR=<path to where pthread.h lies> PTHREAD_LIBRARY=<path to pthread.lib>","title":"4. Generate Visual Studio Solution."},{"location":"getting-started/build-on-windows/#using-vcpkg","text":"","title":"Using vcpkg"},{"location":"getting-started/vcpkg-library-manager/","text":"Vcpkg Library Manager SRT is included in vcpkg ( PR 8712 ). Vcpkg is C++ Library Manager for Windows, Linux, and MacOS. There are two options to build SRT using vcpkg . 1. Build SRT with vcpkg Once SRT is added to the list of supported ports of vcpkg, the buld will be: vcpkg install libsrt 2. Build SRT with openssl and pthreads built with vcpkg At the moment it can already be used to simplify SRT build. Vcpkg already has ports for openssl and pthreads , so the build is as easy as vcpkg install openssl vcpkg install pthreads Integrate vcpkg with the build system (CMake will know about packaged installed by vcpkg) vcpkg integrate install Then build SRT: cmake ../ -G \"Visual Studio 16 2019\" -A x64 -DCMAKE_TOOLCHAIN_FILE=[vcpkg root]\\scripts\\buildsystems\\vcpkg.cmake","title":"Vcpkg Library Manager"},{"location":"getting-started/vcpkg-library-manager/#vcpkg-library-manager","text":"SRT is included in vcpkg ( PR 8712 ). Vcpkg is C++ Library Manager for Windows, Linux, and MacOS. There are two options to build SRT using vcpkg .","title":"Vcpkg Library Manager"},{"location":"getting-started/vcpkg-library-manager/#1-build-srt-with-vcpkg","text":"Once SRT is added to the list of supported ports of vcpkg, the buld will be: vcpkg install libsrt","title":"1. Build SRT with vcpkg"},{"location":"getting-started/vcpkg-library-manager/#2-build-srt-with-openssl-and-pthreads-built-with-vcpkg","text":"At the moment it can already be used to simplify SRT build. Vcpkg already has ports for openssl and pthreads , so the build is as easy as vcpkg install openssl vcpkg install pthreads Integrate vcpkg with the build system (CMake will know about packaged installed by vcpkg) vcpkg integrate install Then build SRT: cmake ../ -G \"Visual Studio 16 2019\" -A x64 -DCMAKE_TOOLCHAIN_FILE=[vcpkg root]\\scripts\\buildsystems\\vcpkg.cmake","title":"2. Build SRT with openssl and pthreads built with vcpkg"},{"location":"protocol/","text":"The Protocol","title":"The Protocol"},{"location":"protocol/#the-protocol","text":"","title":"The Protocol"},{"location":"protocol/bandwidth-estimation/","text":"Bandwidth Estimation SRT estimates the value of the link capacity. This is done by using a packet pair probing technique. In this technique, the sender sends two back-to-back packets of the same size. Once these two packets arrive at the receiver side, there will be an interval between the two packets. The link capacity can then be determined by B = packet size / interval SRT sends out a packet pair every 16 packets. However, when it happens that there is no 17th packet to be sent out, SRT will still send out the 16th packet, rather than waiting for the next one. In this case, there will be a bigger interval (hence underestimation) at the receiver side. The receiver should use a median filter to detect and discard such values. In addition, other patterns of packet pairs can be used, as long as the receiver has a way to identify the packet pairs. Ningning Hu, Peter Steenkiste. Estimating Available Bandwidth Using Packet Pair Probing . 2002. Yunhong Gu. The UDT Congestion Control Algorithm . 2009.","title":"Bandwidth Estimation"},{"location":"protocol/bandwidth-estimation/#bandwidth-estimation","text":"SRT estimates the value of the link capacity. This is done by using a packet pair probing technique. In this technique, the sender sends two back-to-back packets of the same size. Once these two packets arrive at the receiver side, there will be an interval between the two packets. The link capacity can then be determined by B = packet size / interval SRT sends out a packet pair every 16 packets. However, when it happens that there is no 17th packet to be sent out, SRT will still send out the 16th packet, rather than waiting for the next one. In this case, there will be a bigger interval (hence underestimation) at the receiver side. The receiver should use a median filter to detect and discard such values. In addition, other patterns of packet pairs can be used, as long as the receiver has a way to identify the packet pairs. Ningning Hu, Peter Steenkiste. Estimating Available Bandwidth Using Packet Pair Probing . 2002. Yunhong Gu. The UDT Congestion Control Algorithm . 2009.","title":"Bandwidth Estimation"},{"location":"protocol/overhead/","text":"SRT Protocol Overhead :root { --ion-safe-area-top: 20px; --ion-safe-area-bottom: 22px; } function calcOverhead(Mbps, loss_ratio, latency_xrtt) { const bps = Mbps * 1000000; const MAX_PLD = 1316; const DATA_HDR_BYTES = 24; const ACK_BYTES = 44; const ACKACK_BYTES = 16; const NAK_BYTES = 16 + 4; const data_pkts = Math.ceil(bps / (8 * MAX_PLD)); const data_hdr_bytes = data_pkts * DATA_HDR_BYTES; const ack_pkts = 1000 / 10; // Every 10 ms const ack_bytes = ack_pkts * ACK_BYTES; const ackack_bytes = ack_pkts * ACKACK_BYTES; const lost_pkts = Math.ceil(data_pkts * loss_ratio); const nak_bytes = lost_pkts * NAK_BYTES; // TODO: latency of 1RTT const resend_bytes = lost_pkts * (MAX_PLD + DATA_HDR_BYTES) //console.log(\"Bitrate \" + Mbps + \" num pkts \" + data_pkts + \" lost pkts \" + lost_pkts + \" resend \" + resend_bytes); return { dataHdrBytes: data_hdr_bytes, ackBytes: ack_bytes, ackackBytes: ackack_bytes, nakBytes: nak_bytes, resendBytes: resend_bytes }; } function createDataSet(beginMbps, endMbps, stepMbps, lossRatio = 0) { let bitrates = []; let dataHeaderBits = []; let ackBits = []; let ackackBits = []; let nakBits = []; let resendBits = []; // _.range([start], stop, [step]) // _.range(10); // => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] for (let Mbps = beginMbps; Mbps < endMbps; Mbps += stepMbps) { //console.log(Mbps); //const {dataHdrBits, ackBits, ackackBits, nakBits, resendBits} = calcOverhead(Mbps, 0, 0); const {dataHdrBytes, ackBytes, ackackBytes, nakBytes, resendBytes} = calcOverhead(Mbps, lossRatio, 0); bitrates.push(Mbps); dataHeaderBits.push(((8 * dataHdrBytes) / (Mbps * 10000)).toFixed(3)); ackBits.push(((8 * ackBytes) / (Mbps * 10000)).toFixed(3)); ackackBits.push(((8 * ackackBytes) / (Mbps * 10000)).toFixed(3)); nakBits.push(((8 * nakBytes) / (Mbps * 10000)).toFixed(3)); resendBits.push(((8 * resendBytes) / (Mbps * 10000)).toFixed(3)); } //console.log(ackackBits) return { bitratesMbps: bitrates, dataHeaderBits: dataHeaderBits, ackBits: ackBits, ackackBits: ackackBits, nakBits: nakBits, resendBits: resendBits }; } var ctx = document.getElementById(\"stacked-area-chart\").getContext(\"2d\"); const colors = { green: { fill: '#e0eadf', stroke: '#5eb84d', }, lightBlue: { stroke: '#6fccdd', }, darkBlue: { fill: '#92bed2', stroke: '#3282bf', }, purple: { fill: '#8fa8c8', stroke: '#75539e', }, }; function onLossRatioChanged(value) { //console.log(\"New value \" + value); dataSet = createDataSet(1, 30, 1, value / 100); myChart.data.datasets[0].data = dataSet.dataHeaderBits; myChart.data.datasets[1].data = dataSet.ackBits; myChart.data.datasets[2].data = dataSet.ackackBits; myChart.data.datasets[3].data = dataSet.nakBits; myChart.data.datasets[4].data = dataSet.resendBits; //console.log(myChart.data.datasets); myChart.update(); } const dataPackets = Array.from(new Array(5), (val, i)=> calcOverhead(1+ i * 5, 0, 0) ); var dataSet = createDataSet(1, 30, 1); const availableForExisting = [16, 13, 25, 33, 40, 33, 45]; const unavailable = [5, 9, 10, 9, 18, 19, 20]; var myChart = new Chart(ctx, { type: 'line', data: { labels: dataSet.bitratesMbps, datasets: [{ label: \"DATA Header Bits\", fill: true, backgroundColor: colors.purple.fill, pointBackgroundColor: colors.purple.stroke, borderColor: colors.purple.stroke, pointHighlightStroke: colors.purple.stroke, borderCapStyle: 'butt', data: dataSet.dataHeaderBits, }, { label: \"ACK Bits\", fill: true, backgroundColor: colors.darkBlue.fill, pointBackgroundColor: colors.darkBlue.stroke, borderColor: colors.darkBlue.stroke, pointHighlightStroke: colors.darkBlue.stroke, borderCapStyle: 'butt', data: dataSet.ackBits, }, { label: \"ACKACK Bits\", fill: true, backgroundColor: colors.green.fill, pointBackgroundColor: colors.lightBlue.stroke, borderColor: colors.lightBlue.stroke, pointHighlightStroke: colors.lightBlue.stroke, borderCapStyle: 'butt', data: dataSet.ackackBits, }, { label: \"NAK Bits\", fill: true, backgroundColor: colors.green.fill, pointBackgroundColor: colors.green.stroke, borderColor: colors.green.stroke, pointHighlightStroke: colors.green.stroke, data: dataSet.nakBits, }, { label: \"Resend Bits\", fill: true, backgroundColor: colors.green.fill, pointBackgroundColor: colors.green.stroke, borderColor: colors.green.stroke, pointHighlightStroke: colors.green.stroke, data: dataSet.resendBits, }] }, options: { showAllTooltips: true, responsive: false, // Can't just just `stacked: true` like the docs say scales: { yAxes: [{ stacked: true, scaleLabel: { display: true, labelString: 'Overhead, %' } }], xAxes: [{ scaleLabel: { display: true, labelString: 'Bitrate, Mbps' } }] }, animation: { duration: 750, }, } }); Loss ratio: Latency (xRTT): TODO $(\".js-range-slider\").ionRangeSlider({ skin: \"round\", grid: true, min: 0, max: 10, step: 1, from: 0, postfix: \" %\" });","title":"SRT Protocol Overhead"},{"location":"protocol/overhead/#srt-protocol-overhead","text":":root { --ion-safe-area-top: 20px; --ion-safe-area-bottom: 22px; } function calcOverhead(Mbps, loss_ratio, latency_xrtt) { const bps = Mbps * 1000000; const MAX_PLD = 1316; const DATA_HDR_BYTES = 24; const ACK_BYTES = 44; const ACKACK_BYTES = 16; const NAK_BYTES = 16 + 4; const data_pkts = Math.ceil(bps / (8 * MAX_PLD)); const data_hdr_bytes = data_pkts * DATA_HDR_BYTES; const ack_pkts = 1000 / 10; // Every 10 ms const ack_bytes = ack_pkts * ACK_BYTES; const ackack_bytes = ack_pkts * ACKACK_BYTES; const lost_pkts = Math.ceil(data_pkts * loss_ratio); const nak_bytes = lost_pkts * NAK_BYTES; // TODO: latency of 1RTT const resend_bytes = lost_pkts * (MAX_PLD + DATA_HDR_BYTES) //console.log(\"Bitrate \" + Mbps + \" num pkts \" + data_pkts + \" lost pkts \" + lost_pkts + \" resend \" + resend_bytes); return { dataHdrBytes: data_hdr_bytes, ackBytes: ack_bytes, ackackBytes: ackack_bytes, nakBytes: nak_bytes, resendBytes: resend_bytes }; } function createDataSet(beginMbps, endMbps, stepMbps, lossRatio = 0) { let bitrates = []; let dataHeaderBits = []; let ackBits = []; let ackackBits = []; let nakBits = []; let resendBits = []; // _.range([start], stop, [step]) // _.range(10); // => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] for (let Mbps = beginMbps; Mbps < endMbps; Mbps += stepMbps) { //console.log(Mbps); //const {dataHdrBits, ackBits, ackackBits, nakBits, resendBits} = calcOverhead(Mbps, 0, 0); const {dataHdrBytes, ackBytes, ackackBytes, nakBytes, resendBytes} = calcOverhead(Mbps, lossRatio, 0); bitrates.push(Mbps); dataHeaderBits.push(((8 * dataHdrBytes) / (Mbps * 10000)).toFixed(3)); ackBits.push(((8 * ackBytes) / (Mbps * 10000)).toFixed(3)); ackackBits.push(((8 * ackackBytes) / (Mbps * 10000)).toFixed(3)); nakBits.push(((8 * nakBytes) / (Mbps * 10000)).toFixed(3)); resendBits.push(((8 * resendBytes) / (Mbps * 10000)).toFixed(3)); } //console.log(ackackBits) return { bitratesMbps: bitrates, dataHeaderBits: dataHeaderBits, ackBits: ackBits, ackackBits: ackackBits, nakBits: nakBits, resendBits: resendBits }; } var ctx = document.getElementById(\"stacked-area-chart\").getContext(\"2d\"); const colors = { green: { fill: '#e0eadf', stroke: '#5eb84d', }, lightBlue: { stroke: '#6fccdd', }, darkBlue: { fill: '#92bed2', stroke: '#3282bf', }, purple: { fill: '#8fa8c8', stroke: '#75539e', }, }; function onLossRatioChanged(value) { //console.log(\"New value \" + value); dataSet = createDataSet(1, 30, 1, value / 100); myChart.data.datasets[0].data = dataSet.dataHeaderBits; myChart.data.datasets[1].data = dataSet.ackBits; myChart.data.datasets[2].data = dataSet.ackackBits; myChart.data.datasets[3].data = dataSet.nakBits; myChart.data.datasets[4].data = dataSet.resendBits; //console.log(myChart.data.datasets); myChart.update(); } const dataPackets = Array.from(new Array(5), (val, i)=> calcOverhead(1+ i * 5, 0, 0) ); var dataSet = createDataSet(1, 30, 1); const availableForExisting = [16, 13, 25, 33, 40, 33, 45]; const unavailable = [5, 9, 10, 9, 18, 19, 20]; var myChart = new Chart(ctx, { type: 'line', data: { labels: dataSet.bitratesMbps, datasets: [{ label: \"DATA Header Bits\", fill: true, backgroundColor: colors.purple.fill, pointBackgroundColor: colors.purple.stroke, borderColor: colors.purple.stroke, pointHighlightStroke: colors.purple.stroke, borderCapStyle: 'butt', data: dataSet.dataHeaderBits, }, { label: \"ACK Bits\", fill: true, backgroundColor: colors.darkBlue.fill, pointBackgroundColor: colors.darkBlue.stroke, borderColor: colors.darkBlue.stroke, pointHighlightStroke: colors.darkBlue.stroke, borderCapStyle: 'butt', data: dataSet.ackBits, }, { label: \"ACKACK Bits\", fill: true, backgroundColor: colors.green.fill, pointBackgroundColor: colors.lightBlue.stroke, borderColor: colors.lightBlue.stroke, pointHighlightStroke: colors.lightBlue.stroke, borderCapStyle: 'butt', data: dataSet.ackackBits, }, { label: \"NAK Bits\", fill: true, backgroundColor: colors.green.fill, pointBackgroundColor: colors.green.stroke, borderColor: colors.green.stroke, pointHighlightStroke: colors.green.stroke, data: dataSet.nakBits, }, { label: \"Resend Bits\", fill: true, backgroundColor: colors.green.fill, pointBackgroundColor: colors.green.stroke, borderColor: colors.green.stroke, pointHighlightStroke: colors.green.stroke, data: dataSet.resendBits, }] }, options: { showAllTooltips: true, responsive: false, // Can't just just `stacked: true` like the docs say scales: { yAxes: [{ stacked: true, scaleLabel: { display: true, labelString: 'Overhead, %' } }], xAxes: [{ scaleLabel: { display: true, labelString: 'Bitrate, Mbps' } }] }, animation: { duration: 750, }, } });","title":"SRT Protocol Overhead"},{"location":"protocol/congestion-control/","text":"Congestion Control Starting from v1.3.0, SRT provides two types of data transmission: live and file. The mode can be selected via the socket option SRTO_CONGESTION (or more general SRTO_TRANSTYPE option). Useful Links Yunhong Gu. The UDT Congestion Control Algorithm . 2009. Network Congestion Management: Considerations and Techniques","title":"Congestion Control"},{"location":"protocol/congestion-control/#congestion-control","text":"Starting from v1.3.0, SRT provides two types of data transmission: live and file. The mode can be selected via the socket option SRTO_CONGESTION (or more general SRTO_TRANSTYPE option).","title":"Congestion Control"},{"location":"protocol/congestion-control/#useful-links","text":"Yunhong Gu. The UDT Congestion Control Algorithm . 2009. Network Congestion Management: Considerations and Techniques","title":"Useful Links"},{"location":"protocol/congestion-control/file-cc/","text":"File CC The congestion control in SRT mainly relies on the value called SndPeriod, it's the required minimum time gap between sending two consecutive packets (in [us]). Slow Start The congestion control in SRT mainly relies on the value called SndPeriod, it's the required minimum time gap between sending two consecutive packets (in [us]). It's not the waiting time between packets, it's rather the time expected to be between consecutive sending, that is, when the time at the moment of the packet to be sent is distant to the previous sending time by less than SndPeriod, the procedure is sleeping for the \"remaining time\", otherwise the packet is sent immediately. The whole congestion control relies on setting appropriate value to SndPeriod in order to achieve appropriate sending speed. The original UDT algorithm (implemented by FileSmoother) relies on the following rules: Congestion Avoidance","title":"File CC"},{"location":"protocol/congestion-control/file-cc/#file-cc","text":"The congestion control in SRT mainly relies on the value called SndPeriod, it's the required minimum time gap between sending two consecutive packets (in [us]).","title":"File CC"},{"location":"protocol/congestion-control/file-cc/#slow-start","text":"The congestion control in SRT mainly relies on the value called SndPeriod, it's the required minimum time gap between sending two consecutive packets (in [us]). It's not the waiting time between packets, it's rather the time expected to be between consecutive sending, that is, when the time at the moment of the packet to be sent is distant to the previous sending time by less than SndPeriod, the procedure is sleeping for the \"remaining time\", otherwise the packet is sent immediately. The whole congestion control relies on setting appropriate value to SndPeriod in order to achieve appropriate sending speed. The original UDT algorithm (implemented by FileSmoother) relies on the following rules:","title":"Slow Start"},{"location":"protocol/congestion-control/file-cc/#congestion-avoidance","text":"","title":"Congestion Avoidance"},{"location":"protocol/tsbpd/","text":"TSBPD","title":"TSBPD"},{"location":"protocol/tsbpd/#tsbpd","text":"","title":"TSBPD"},{"location":"protocol/tsbpd/drift-management/","text":"Drift Management When the sender enters \u201cconnected\u201d state it tells the application there is a socket interface that is transmitter-ready. At this point the application can start sending data packets. It adds packets to the SRT sender\u2018s buffer at a certain input rate, from which they are transmitted to the receiver at scheduled times. A synchronized time is required to keep proper sender/receiver buffer levels, taking into account the time zone and round-trip time (up to 2 seconds for satellite links). Considering addition/subtraction round-\u00adoff, and possibly unsynchronized system times, an agreed-\u00adupon time base drifts by a few microseconds every minute. The drift may accumulate over many days to a point where the sender or receiver buffers will overflow or deplete, seriously affecting the quality of the video. SRT has a time management mechanism to compensate for this drift. When a packet is received, SRT determines the difference between the time it was expected and its timestamp. The timestamp is calculated on the receiver side. The RTT tells the receiver how much time it was supposed to take. SRT maintains a reference between the time at the leading edge of the send buffer\u2018s latency window and the corresponding time on the receiver (the present time). This allows conversion to real time to be able to schedule events, based on a local time reference. The receiver samples time drift data and periodically calculates a packet timestamp correction factor, which is applied to each data packet received by adjusting the inter-packet interval. When a packet is received it isn\u2018t given right away to the application. As time advances, the receiver knows the expected time for any missing or dropped packet, and can use this information to fill any \u201choles\u201d in the receive queue with another packet. The receiver uses local time to be able to schedule events \u2014 to determine, for example, if it\u2018s time to deliver a certain packet right away. The timestamps in the packets themselves are just references to the beginning of the session. When a packet is received (with a timestamp from the sender), the receiver makes a reference to the beginning of the session to recalculate its timestamp. The start time is derived from the local time at the moment that the session is connected. A packet timestamp equals \u201cnow\u201d minus \u201cStartTime\u201d, where the latter is the point in time when the socket was created.","title":"Drift Management"},{"location":"protocol/tsbpd/drift-management/#drift-management","text":"When the sender enters \u201cconnected\u201d state it tells the application there is a socket interface that is transmitter-ready. At this point the application can start sending data packets. It adds packets to the SRT sender\u2018s buffer at a certain input rate, from which they are transmitted to the receiver at scheduled times. A synchronized time is required to keep proper sender/receiver buffer levels, taking into account the time zone and round-trip time (up to 2 seconds for satellite links). Considering addition/subtraction round-\u00adoff, and possibly unsynchronized system times, an agreed-\u00adupon time base drifts by a few microseconds every minute. The drift may accumulate over many days to a point where the sender or receiver buffers will overflow or deplete, seriously affecting the quality of the video. SRT has a time management mechanism to compensate for this drift. When a packet is received, SRT determines the difference between the time it was expected and its timestamp. The timestamp is calculated on the receiver side. The RTT tells the receiver how much time it was supposed to take. SRT maintains a reference between the time at the leading edge of the send buffer\u2018s latency window and the corresponding time on the receiver (the present time). This allows conversion to real time to be able to schedule events, based on a local time reference. The receiver samples time drift data and periodically calculates a packet timestamp correction factor, which is applied to each data packet received by adjusting the inter-packet interval. When a packet is received it isn\u2018t given right away to the application. As time advances, the receiver knows the expected time for any missing or dropped packet, and can use this information to fill any \u201choles\u201d in the receive queue with another packet. The receiver uses local time to be able to schedule events \u2014 to determine, for example, if it\u2018s time to deliver a certain packet right away. The timestamps in the packets themselves are just references to the beginning of the session. When a packet is received (with a timestamp from the sender), the receiver makes a reference to the beginning of the session to recalculate its timestamp. The start time is derived from the local time at the moment that the session is connected. A packet timestamp equals \u201cnow\u201d minus \u201cStartTime\u201d, where the latter is the point in time when the socket was created.","title":"Drift Management"},{"location":"protocol/tsbpd/latency/","text":"SRT Latency SRT has an end-to-end latency between the time a packet is given to SRT ( srt_sendmsg(...) ) and the time this very packet is received from SRT ( srt_recvmsg(...) ). The timing diagram issulstrates those key latency points with TSBPD enabled (live mode). !!! warning \"Latency confustion\" The docs on SRTO_RCVLATENCY say it sets \"the time that should elapse since the moment when the packet was sent and the moment when it is delivered to the receiver application in the receiving function.\" But the actual latency on the link will roughly be SRTO_RCVLATENCY + RTT/2 Packet Delivery Time Packet delivery time is the time point, estimated by the receiver, when a packet should be given (delivered) to the upstream application (via srt_recvmsg(...) ). It consists of the TsbPdTimeBase - the base time PktTsbPdTime = TsbPdTimeBase + TsbPdDelay + PKT_TIMESTAMP + Drift TSBPD Base Time TsbPdTimeBase is the base time difference between local clock of the receiver, and the clock used by the sender to timestamp packets being sent. A unit of measurement is microseconds. Initial value The value of TsbPdTimeBase is initialized at the time of the handshake request ( HSREQ ) is received. TsbPdTimeBase = T_NOW - HSREQ_TIMESTAMP . This value should roughly correspond to the one-way delay ( ~RTT/2 ). TSBPD Wrapping Period The value of TsbPdTimeBase can be updated during the TSBPD wrapping period. The period starts 30 seconds before reaching the maximum timestamp value of a packet ( CPacket::MAX_TIMESTAMP ), and ends whens the timestamp of the received packet is within [30; 60] seconds. CPacket::MAX_TIMESTAMP = 0xFFFFFFFF , or maximum 32-bit unsigned integer value. The value is in microseconds, which corresponds to 1 hour 11 minutes and 35 seconds (01:11:35). In other words, TSBPD time wrapping happens every 01:11:35. During this wrapping period, a packet may have a timestamp close to CPacket::MAX_TIMESTAMP , as well as close to 0 . Both cases are handled. In the first case, the current value of TsbPdTimeBase is used. In the seconds case, TsbPdTimeBase + CPacket::MAX_TIMESTAMP + 1 is used to calculate TSBPD time of a packet. The wrapping period ends when the timestamp of the received packet is within the interval [30; 60] seconds. The updated value will be TsbPdTimeBase += CPacket::MAX_TIMESTAMP + 1 . Time Drift The value of TsbPdTimeBase can be updated by the DriftTracer. Time Drift Sample Upon receipt of an ACKACK packet, the timestamp of this control packet is used as a sample for drift tracing. ACKACK timestamp is expected to be half the round-trip time ago ( RTT/2 ). The drift time DRIFT is calculated from the current time T_NOW ; the TSBPD base time TsbPdTimeBase ; and the timestamp ACKACK_TIMESTAMP of the received ACKACK packet. DRIFT = T_NOW - (TsbPdTimeBase + ACKACK_TIMESTAMP) The base time should stay in sync with T_NOW - T_SENDER , and should roughly correspond to ~RTT/2 . The value of ACKACK_TIMESTAMP should represent T_SENDER , and be ~RTT/2 in the past. Therefore, the above equation can be considered as DRIFT = T_NOW - (T_NOW - T_SENDER + T_SENDER) -> 0 if the link latency remains constant. Assuming that the link latency is constant (RTT=const), the only cause of the drift fluctuations should be clock inaccuracy. Drift Tracing and Adjustment Drift tracing is based on accumulating the sum of drift samples. DriftSum - the sum of the time drift samples on a MAX_SPAN number of samples. DriftSpan is the current number of accumulated samples. The default value of MAX_SPAN is 1000 samples. The default value of MAX_DRIFT is 5000 \u03bcs (5 ms). The default value of CLEAR_ON_UPDATE is true . On each DriftSpan sample, the average drift value Drift is updated as Drift = DriftSum / DriftSpan . The values of DriftSpan and DriftSum are reset to 0. If the absolute value of the Drift exceeds MAX_DRIFT ( |Drift| > MAX_DRIFT ), the remainder goes to OverDrift value. The value of OverDrift is used to update the TsbPdTimeBase . In pseudo-code it looks like this: bool update(int64_t driftval) { DriftSum += driftval; ++DriftSpan; if (m_uDriftSpan < MAX_SPAN) return false; if (CLEAR_ON_UPDATE) Overdrift = 0; Drift = DriftSum / DriftSpan; DriftSum = 0; DriftSpan = 0; if (std::abs(Drift) > MAX_DRIFT) { Overdrift = Drift < 0 ? -MAX_DRIFT : MAX_DRIFT; Drift -= Overdrift; } // Drift value was updated return true; } TODO: Use RTTVar. Class DriftTracer The DriftTracer class has the following prototype. template<unsigned MAX_SPAN, int MAX_DRIFT, bool CLEAR_ON_UPDATE = true> class DriftTracer { public: DriftTracer(); public: bool update(int64_t driftval); int64_t drift() const; int64_t overdrift() const; };","title":"SRT Latency"},{"location":"protocol/tsbpd/latency/#srt-latency","text":"SRT has an end-to-end latency between the time a packet is given to SRT ( srt_sendmsg(...) ) and the time this very packet is received from SRT ( srt_recvmsg(...) ). The timing diagram issulstrates those key latency points with TSBPD enabled (live mode). !!! warning \"Latency confustion\" The docs on SRTO_RCVLATENCY say it sets \"the time that should elapse since the moment when the packet was sent and the moment when it is delivered to the receiver application in the receiving function.\" But the actual latency on the link will roughly be SRTO_RCVLATENCY + RTT/2","title":"SRT Latency"},{"location":"protocol/tsbpd/latency/#packet-delivery-time","text":"Packet delivery time is the time point, estimated by the receiver, when a packet should be given (delivered) to the upstream application (via srt_recvmsg(...) ). It consists of the TsbPdTimeBase - the base time PktTsbPdTime = TsbPdTimeBase + TsbPdDelay + PKT_TIMESTAMP + Drift","title":"Packet Delivery Time"},{"location":"protocol/tsbpd/latency/#tsbpd-base-time","text":"TsbPdTimeBase is the base time difference between local clock of the receiver, and the clock used by the sender to timestamp packets being sent. A unit of measurement is microseconds.","title":"TSBPD Base Time"},{"location":"protocol/tsbpd/latency/#initial-value","text":"The value of TsbPdTimeBase is initialized at the time of the handshake request ( HSREQ ) is received. TsbPdTimeBase = T_NOW - HSREQ_TIMESTAMP . This value should roughly correspond to the one-way delay ( ~RTT/2 ).","title":"Initial value"},{"location":"protocol/tsbpd/latency/#tsbpd-wrapping-period","text":"The value of TsbPdTimeBase can be updated during the TSBPD wrapping period. The period starts 30 seconds before reaching the maximum timestamp value of a packet ( CPacket::MAX_TIMESTAMP ), and ends whens the timestamp of the received packet is within [30; 60] seconds. CPacket::MAX_TIMESTAMP = 0xFFFFFFFF , or maximum 32-bit unsigned integer value. The value is in microseconds, which corresponds to 1 hour 11 minutes and 35 seconds (01:11:35). In other words, TSBPD time wrapping happens every 01:11:35. During this wrapping period, a packet may have a timestamp close to CPacket::MAX_TIMESTAMP , as well as close to 0 . Both cases are handled. In the first case, the current value of TsbPdTimeBase is used. In the seconds case, TsbPdTimeBase + CPacket::MAX_TIMESTAMP + 1 is used to calculate TSBPD time of a packet. The wrapping period ends when the timestamp of the received packet is within the interval [30; 60] seconds. The updated value will be TsbPdTimeBase += CPacket::MAX_TIMESTAMP + 1 .","title":"TSBPD Wrapping Period"},{"location":"protocol/tsbpd/latency/#time-drift","text":"The value of TsbPdTimeBase can be updated by the DriftTracer.","title":"Time Drift"},{"location":"protocol/tsbpd/latency/#time-drift-sample","text":"Upon receipt of an ACKACK packet, the timestamp of this control packet is used as a sample for drift tracing. ACKACK timestamp is expected to be half the round-trip time ago ( RTT/2 ). The drift time DRIFT is calculated from the current time T_NOW ; the TSBPD base time TsbPdTimeBase ; and the timestamp ACKACK_TIMESTAMP of the received ACKACK packet. DRIFT = T_NOW - (TsbPdTimeBase + ACKACK_TIMESTAMP) The base time should stay in sync with T_NOW - T_SENDER , and should roughly correspond to ~RTT/2 . The value of ACKACK_TIMESTAMP should represent T_SENDER , and be ~RTT/2 in the past. Therefore, the above equation can be considered as DRIFT = T_NOW - (T_NOW - T_SENDER + T_SENDER) -> 0 if the link latency remains constant. Assuming that the link latency is constant (RTT=const), the only cause of the drift fluctuations should be clock inaccuracy.","title":"Time Drift Sample"},{"location":"protocol/tsbpd/latency/#drift-tracing-and-adjustment","text":"Drift tracing is based on accumulating the sum of drift samples. DriftSum - the sum of the time drift samples on a MAX_SPAN number of samples. DriftSpan is the current number of accumulated samples. The default value of MAX_SPAN is 1000 samples. The default value of MAX_DRIFT is 5000 \u03bcs (5 ms). The default value of CLEAR_ON_UPDATE is true . On each DriftSpan sample, the average drift value Drift is updated as Drift = DriftSum / DriftSpan . The values of DriftSpan and DriftSum are reset to 0. If the absolute value of the Drift exceeds MAX_DRIFT ( |Drift| > MAX_DRIFT ), the remainder goes to OverDrift value. The value of OverDrift is used to update the TsbPdTimeBase . In pseudo-code it looks like this: bool update(int64_t driftval) { DriftSum += driftval; ++DriftSpan; if (m_uDriftSpan < MAX_SPAN) return false; if (CLEAR_ON_UPDATE) Overdrift = 0; Drift = DriftSum / DriftSpan; DriftSum = 0; DriftSpan = 0; if (std::abs(Drift) > MAX_DRIFT) { Overdrift = Drift < 0 ? -MAX_DRIFT : MAX_DRIFT; Drift -= Overdrift; } // Drift value was updated return true; } TODO: Use RTTVar.","title":"Drift Tracing and Adjustment"},{"location":"protocol/tsbpd/latency/#class-drifttracer","text":"The DriftTracer class has the following prototype. template<unsigned MAX_SPAN, int MAX_DRIFT, bool CLEAR_ON_UPDATE = true> class DriftTracer { public: DriftTracer(); public: bool update(int64_t driftval); int64_t drift() const; int64_t overdrift() const; };","title":"Class DriftTracer"}]}